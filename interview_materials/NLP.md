# NLP 面试题

## 1. 什么是NLP？

​        自然语言处理（Natural Language Processing）是人工智能（AI）的一个子领域（AI技术包括大数据、计算机视觉、语音识别、自然语言处理和机器学习五大部分）。

​        NLP是研究人与人交互中以及在人与计算机交互中的语言问题的一门科学。为了建设和完善语言模型，NLP建立计算框架，提出相应的方法来不断的完善设计各种实用系统，并探讨这些实用系统的评测方法。

## 2. NLP的主要研究方向

### （1）信息抽取

​        从给定文本中抽取重要信息，比如时间、地点、任务、时间、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因对谁、做了什么事、有什么后果。

### （2）文本生成

​        机器像人一样使用自然语言进行表达和写作。依据输入的不同，文本生成技术主演包括数据到文本生成和文本到文本生成。

​        数据到文本生成是指将包含键值对的数据转化为自然语言文本；文本到文本生成是对输入文本进行转化和处理从而产生新的文本。

### （3）问答系统

​        对于一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别、形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳答案。

### （4）对话系统

​        系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。

### （5）文本挖掘

​        包括文本聚类、分类、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。

### （6）语音识别和生成

​        语音识别是将输入计算机的语音符号识别转换成书面语表示。语音生成又称文语转换、语音合成，它是指将书面文本自动转换成对应的语音表征。

### （7）信息过滤

​        通过计算机系统自动识别和过滤符合特定条件的文档信息。通常指网络有害信息的自动识别和过滤，主要用于信息安全和防护，网络内容管理等。

### （8）舆情分析

​        指收集和处理海量信息，自动化地对网络舆情进行分析，以实现及时应对网络舆情的目的。

### （9）信息检索

​        对大规模的文档进行索引。

​        可简单对文档中的词汇，赋之以不同的权重来建立索引，也可建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的稳定。

### （10）机器翻译

​        把输入的源语言文本通过自动翻译获得另外一种语言的文本。机器翻译从最早的基于规则的方法到20年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。

## 3. 处理NLP任务的一般步骤

如下图总结：

![](https://upload-images.jianshu.io/upload_images/1667471-37315f7baaee75f4.jpg)

一般流程按照上图列举明细如下：

### （1）获取语料

1. #### 语料定义：

   是NLP任务研究的内容。通常用一个文本集合作为语料库（Corpus）

2. #### 来源：

   a. 已有语料（积累的文档）

   b. 下载语料（搜狗语料、人民日报语料）

   c. 抓取语料

### （2）语料预处理

1. ####  语料清晰

   - 解释：留下有用的，删除噪音数据
   - 常见的数据清晰方式：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。

2. ####  分词

   - 解释：将文本分成词语
   - 常见的分词算法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法

3. ####  词性标注

   - 解释：给词语打词类标签，如形容词、动词、名词等，这在情感分析和知识推理等任务中需要
   - 常见的词性标注方法：基于规则和基于统计（如基于最大熵的词性标注、基于统计最大概率输出词性标注和基于HMM的词性标注）

4. ####  去停用词

   去掉对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等。

### （3）特征工程

### （4）特征选择

### （5）模型训练

### （6）评价指标

### （7）模型上线应用

## 4. 中文分词技术

### 4.1 简介

1.  词是最小的能够独立活动的有意义的语言成分。
2. 

### 4.2 分词方法

1. **基于规则分词**

   - 正向最大匹配法（Maximum Match Method, MM法）

   - 逆向最大匹配法（Inverse Maximum Match Method，IMM法）

     由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精度。所以IMM比MM的误差要小。

   - 双向最大匹配法

     双向最大匹配规则：

     1. 如果正反分词结果次数不同，则取分词数量较少的那个（如“南京市/长江/大桥”的分词数量为3，而“南京市/长江大桥”的分词数量为2，所以返回分词数量为2的）
     2. 如果分词结果次数相同：
        - 分词结果相同，就说明没有歧义，可返回任意一个。
        - 分词结果不同，返回其中单字较少的那个。如对“研究生命的起源”进行分词，MM法分词结果为“['研究生---','命---','的---','起源---',]”，其中单字个数为2个；而IMM法分词结果为“['研究---','生命---','的---','起源---']”，其中单字个数为1个。所以返回IMM的分词结果。

   - 基于规则分词的优缺点：

     1. 优点：简单高效
     2. 缺点：词典维护是一个很庞大的工程，网络新词也是层出不穷，很难覆盖到所有词。

   - 

     

2. **基于统计分词**

   2.1 语言模型：n-gram模型，拉普拉斯平滑算法，解决分子分母为0的问题。

   2.2 分词算法：HMM和CRF等。以可采用深度学习网络如CNN、LSTM来发现一些模式和特征。

3. **基于规则+统计分词**

   实际工程应用中，最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。

   

### 4.3 分词工具jieba

1. 简介

   是先基于词典的方式进行分词，然后再用统计分词方法进行辅助的实现。功能：分词，以及分词之上的关键词提取和词性标注等。

2. 分词模式

   - 精确模式：将句子最精确的切开，适合文本分词。默认模式。jieba.cut(sent, cut_all=False)或者jieba.cut(sent)
   - 全模式：将句子中所有可以成词的词语都扫描出来，速度快，但不能解决歧义。jieba.cut(sent, cut_all=True)
   - 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适用于搜索引擎分词。jieba.cut_for_search(sent)
   - 

3. jieba常用方法

   - **添加自定义词典:**

     ```python
     '''
     两类：载入词典和调整词典
     '''
     (1)载入词典
     # 加载系统词典
     jieba.set_dictionary('D:/python_projects/NLP_Learning_data/chineseCutWord/data/dict.txt.big')
     # 加载用户自定义词典
     jieba.load_userdict('D:/python_projects/NLP_Learning_data/chineseCutWord/data/user_dict.utf8')
     (2)调整词典
     # 调整词典,向词典中添加一个词。freq 和 tag 可以省略，freq 默认为一个计算值
     jieba.add_word(word, freq=None, tag=None)
     # 在词典中删除一个词
     jieba.del_word(word)
     '''
     使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其（或不能）被分出来。注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。
     示例：jieba.suggest_freq(('证件照片'), tune=True)
     '''
     jieba.suggest_freq(segment, tune=True)
     
     ```

   - **词典格式：**

     每一行为三个部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。该词典文件需为utf-8编码。

   - **cut()和lcut()：**

     ```python
     '''
     此方法接受三个参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型;
     lcut 方法直接返回 list，cut 方法返回一个 可迭代的 generator
     '''
     sent = '中文分词是文本处理不可或缺的一步！'
     print('cut is:', jieba.cut(sent))
     结果为：cut is: <generator object Tokenizer.cut at 0x02589530>
     print('精确模式：', '/'.join(jieba.cut(sent)))
     结果为：精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！
     print('lcut is:', jieba.lcut(sent))
     结果为：lcut is: ['中文', '分词', '是', '文本处理', '不可或缺', '的', '一步', '！']
     ```

     

   - **jieba.analyse:**关键词提取

     ```python
     (1). 基于 TF-IDF（term frequency–inverse document frequency） 算法的关键词抽取。
     # 1. 新建TFIDF实例，idf_path为IDF频率文件，可不填
     jieba.analyse.TFIDF(idf_path=None) 
     # 2. 关键词提取所使用的的逆向文件频率（即IDF）文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_idf_path(filename)
     # 3. 关键词提取所用到的停用词文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_stop_words(filename)
     # 4. 开始提取关键字
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回几个 TF/IDF 权重最大的关键词，默认值为 20
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，默认为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
     
     
     (2).是基于 TextRank 算法的关键词抽取。
     # 1. 新建TextRank实例
     jieba.analyse.TextRank()
     # 2. 提取关键词
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回关键词的数量，重要性从高到低排序
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords =jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’))
     
     # 打印结果：
     for item in keywords:
         # 分别为关键词和相应的权重
         print(item[0], item[1])
     或者
     for w,t in keywords:
         print('%s, %s'%(w, t))
     ```

   - 

4. 



## 5. 词性标注



## 6.命名实体识别