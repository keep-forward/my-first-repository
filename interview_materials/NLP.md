# NLP 面试题

## 1. 什么是NLP？

​        自然语言处理（Natural Language Processing）是人工智能（AI）的一个子领域（AI技术包括大数据、计算机视觉、语音识别、自然语言处理和机器学习五大部分）。

​        NLP是研究人与人交互中以及在人与计算机交互中的语言问题的一门科学。为了建设和完善语言模型，NLP建立计算框架，提出相应的方法来不断的完善设计各种实用系统，并探讨这些实用系统的评测方法。

## 2. NLP的主要研究方向

### （1）信息抽取

​        从给定文本中抽取重要信息，比如时间、地点、任务、时间、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因对谁、做了什么事、有什么后果。

### （2）文本生成

​        机器像人一样使用自然语言进行表达和写作。依据输入的不同，文本生成技术主演包括数据到文本生成和文本到文本生成。

​        数据到文本生成是指将包含键值对的数据转化为自然语言文本；文本到文本生成是对输入文本进行转化和处理从而产生新的文本。

### （3）问答系统

​        对于一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别、形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳答案。

### （4）对话系统

​        系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。

### （5）文本挖掘

​        包括文本聚类、分类、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。

### （6）语音识别和生成

​        语音识别是将输入计算机的语音符号识别转换成书面语表示。语音生成又称文语转换、语音合成，它是指将书面文本自动转换成对应的语音表征。

### （7）信息过滤

​        通过计算机系统自动识别和过滤符合特定条件的文档信息。通常指网络有害信息的自动识别和过滤，主要用于信息安全和防护，网络内容管理等。

### （8）舆情分析

​        指收集和处理海量信息，自动化地对网络舆情进行分析，以实现及时应对网络舆情的目的。

### （9）信息检索

​        对大规模的文档进行索引。

​        可简单对文档中的词汇，赋之以不同的权重来建立索引，也可建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的稳定。

### （10）机器翻译

​        把输入的源语言文本通过自动翻译获得另外一种语言的文本。机器翻译从最早的基于规则的方法到20年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。

## 3. 处理NLP任务的一般步骤

如下图总结：

![](https://upload-images.jianshu.io/upload_images/1667471-37315f7baaee75f4.jpg)

一般流程按照上图列举明细如下：

### （1）获取语料

1. #### 语料定义：

   是NLP任务研究的内容。通常用一个文本集合作为语料库（Corpus）

2. #### 来源：

   a. 已有语料（积累的文档）

   b. 下载语料（搜狗语料、人民日报语料）

   c. 抓取语料

### （2）语料预处理

1. ####  语料清晰

   - 解释：留下有用的，删除噪音数据
   - 常见的数据清晰方式：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。

2. ####  分词

   - 解释：将文本分成词语
   - 常见的分词算法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法

3. ####  词性标注

   - 解释：给词语打词类标签，如形容词、动词、名词等，这在情感分析和知识推理等任务中需要
   - 常见的词性标注方法：基于规则和基于统计（如基于最大熵的词性标注、基于统计最大概率输出词性标注和基于HMM的词性标注）

4. ####  去停用词

   去掉对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等。

### （3）特征工程

### （4）特征选择

### （5）模型训练

### （6）评价指标

### （7）模型上线应用

## 4. 中文分词技术

### 4.1 简介

1.  词是最小的能够独立活动的有意义的语言成分。
2. 

### 4.2 分词方法

1. **基于规则分词**

   - 正向最大匹配法（Maximum Match Method, MM法）

   - 逆向最大匹配法（Inverse Maximum Match Method，IMM法）

     由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精度。所以IMM比MM的误差要小。

   - 双向最大匹配法

     双向最大匹配规则：

     1. 如果正反分词结果次数不同，则取分词数量较少的那个（如“南京市/长江/大桥”的分词数量为3，而“南京市/长江大桥”的分词数量为2，所以返回分词数量为2的）
     2. 如果分词结果次数相同：
        - 分词结果相同，就说明没有歧义，可返回任意一个。
        - 分词结果不同，返回其中单字较少的那个。如对“研究生命的起源”进行分词，MM法分词结果为“['研究生---','命---','的---','起源---',]”，其中单字个数为2个；而IMM法分词结果为“['研究---','生命---','的---','起源---']”，其中单字个数为1个。所以返回IMM的分词结果。

   - 基于规则分词的优缺点：

     1. 优点：简单高效
     2. 缺点：词典维护是一个很庞大的工程，网络新词也是层出不穷，很难覆盖到所有词。

   - 

     

2. **基于统计分词**

   2.1 语言模型：n-gram模型，拉普拉斯平滑算法，解决分子分母为0的问题。

   2.2 分词算法：HMM和CRF等。以可采用深度学习网络如CNN、LSTM来发现一些模式和特征。

   - HMM分词是字作为字符串中的序列标注任务来实现的。其基本思路是：每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位），现规定每个字最多只有四个词位：即B（词首）、M（词中）、E（词尾）、S（单独成词）。

   

3. **基于规则+统计分词**

   实际工程应用中，最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。

   

### 4.3 分词工具jieba

1. 简介

   是先基于词典的方式进行分词，然后再用统计分词方法进行辅助的实现。功能：分词，以及分词之上的关键词提取和词性标注等。

2. 分词模式

   - 精确模式：将句子最精确的切开，适合文本分词。默认模式。jieba.cut(sent, cut_all=False)或者jieba.cut(sent)
   - 全模式：将句子中所有可以成词的词语都扫描出来，速度快，但不能解决歧义。jieba.cut(sent, cut_all=True)
   - 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适用于搜索引擎分词。jieba.cut_for_search(sent)
   - 

3. jieba常用方法

   - **添加自定义词典:**

     ```python
     '''
     两类：载入词典和调整词典
     '''
     (1)载入词典
     # 加载系统词典
     jieba.set_dictionary('D:/python_projects/NLP_Learning_data/chineseCutWord/data/dict.txt.big')
     # 加载用户自定义词典
     jieba.load_userdict('D:/python_projects/NLP_Learning_data/chineseCutWord/data/user_dict.utf8')
     (2)调整词典
     # 调整词典,向词典中添加一个词。freq 和 tag 可以省略，freq 默认为一个计算值
     jieba.add_word(word, freq=None, tag=None)
     # 在词典中删除一个词
     jieba.del_word(word)
     '''
     使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其（或不能）被分出来。注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。
     示例：jieba.suggest_freq(('证件照片'), tune=True)
     '''
     jieba.suggest_freq(segment, tune=True)
     
     ```

   - **词典格式：**

     每一行为三个部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。该词典文件需为utf-8编码。

   - **cut()和lcut()：**

     ```python
     '''
     此方法接受三个参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型;
     lcut 方法直接返回 list，cut 方法返回一个 可迭代的 generator
     '''
     sent = '中文分词是文本处理不可或缺的一步！'
     print('cut is:', jieba.cut(sent))
     结果为：cut is: <generator object Tokenizer.cut at 0x02589530>
     print('精确模式：', '/'.join(jieba.cut(sent)))
     结果为：精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！
     print('lcut is:', jieba.lcut(sent))
     结果为：lcut is: ['中文', '分词', '是', '文本处理', '不可或缺', '的', '一步', '！']
     ```

     

   - **jieba.analyse:**关键词提取

     ```python
     (1). 基于 TF-IDF（term frequency–inverse document frequency） 算法的关键词抽取。
     # 1. 新建TFIDF实例，idf_path为IDF频率文件，可不填
     jieba.analyse.TFIDF(idf_path=None) 
     # 2. 关键词提取所使用的的逆向文件频率（即IDF）文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_idf_path(filename)
     # 3. 关键词提取所用到的停用词文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_stop_words(filename)
     # 4. 开始提取关键字
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回几个 TF/IDF 权重最大的关键词，默认值为 20
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，默认为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
     
     
     (2).是基于 TextRank 算法的关键词抽取。
     # 1. 新建TextRank实例
     jieba.analyse.TextRank()
     # 2. 提取关键词
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回关键词的数量，重要性从高到低排序
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords =jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’))
     
     # 打印结果：
     for item in keywords:
         # 分别为关键词和相应的权重
         print(item[0], item[1])
     或者
     for w,t in keywords:
         print('%s, %s'%(w, t))
     ```

   - 

4. 

### 4.4 nltk（Natural Language Toolkit）分词

1.  **简介：**

   nltk是一个高效的python构建平台，用来处理自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如wordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库。

2. **说明：**

   - nltk本身自带了大量的语料，但**大部分是英文的**。

   - nltk自带了很多统计的功能，但针对中文来讲，**分词的工作需要我们手动来完成**。然后再把处理过的文本封装成nltk的“text”对象，之后才能使用nltk进行处理。

     代码举例：

     ```python
     import jieba
     import nltk
     import re
     word="盗墓不是请客吃饭，不是做文章，不是绘画绣花，不能那样雅致，那样从容不迫，文质彬彬，那样温良恭俭让。"
     # 去除标点符号
     cleaned_data = ''.join(re.findall(r'[\u4e00-\u9fa5]', word))
     # lcut 方法直接返回 list
     wordlist = jieba.lcut(cleaned_data)
     # 封装成nltk的“text”对象
     text = nltk.Text(wordlist)
     print(text)
     # 查看单词的上下文
     text.concordance(word='僵尸', width=20, lines=10)
     # 搜索共同上下文
     text.common_contexts(['僵尸', '鬼'])
     # 统计词频
     text.count(word='胖子')
     # 绘制离散图
     words = ['鬼', '僵尸', '胖子', '老胡']
     text.dispersion_plot(words)
     ```

   - **nltk分词基本针对英文的，中文分词主要使用jieba分词**。

   - **下载语料、预训练模型等**（使用前需要首先执行以下代码）。 除了一些个人数据包还可以下载整个集合（使用“all”），或者仅下载书中例子和练习中使用到的数据（使用“book”），或者仅下载没有语法和训练模型的语料库（使用“all-corpora”） .

     ```python
     import nltk
     nltk.download()
     ```

   - 

3. **代码实现：**

   ```python
   import nltk
   import re
   english='H:\\自然语言处理\\Experiment2\\English.txt'
   with open(english,'r',encoding='utf-8') as file:    
       u=file.read()
   str=re.sub('[^\w ]','',u)
   # 分词
   print(nltk.word_tokenize(str))
   # 对分完词的结果进行词性标注
   print(nltk.pos_tag(nltk.word_tokenize(str)))
   ```

4. 



## 5. 词性标注

### 5.1 简介

1. 词性标注是确定给定句子中的每个词的词性（名词n、动词v、形容词a、副词d、助词u、代词等）并加以标注的过程。
2. 

### 5.2 词性标注规范

1. 中文领域中尚无统一的标注标准。较为主流的是北大词性标注集合宾州词性标注集。

### 5.3 词性标注方法

1. **方法：**

   - **jieba.posseg**

     示例：

     ```python
     import jieba.posseg as psg
     sent = '中文分词是文本处理不可或缺的一步！'
     seg_list = psg.cut(sent)
     print(' '.join(['{0}/{1}'.format(w,t) for w,t in seg_list]))
     结果：
     中文/nz 分词/n 是/v 文本处理/n 不可或缺/l 的/uj 一步/m ！/x
     ```

   - **HMM进行词性标注**

     示例：（待完成）

   - **nltk词性标注：**先分词再词性标注

     ```python
     import nltk
     import re
     english='H:\\自然语言处理\\Experiment2\\English.txt'
     with open(english,'r',encoding='utf-8') as file:
         u=file.read()
     str=re.sub('[^\w ]','',u)
     # 分词
     print(nltk.word_tokenize(str))
     # 对分完词的结果进行词性标注
     print(nltk.pos_tag(nltk.word_tokenize(str)))
     ```

   - 

2. **注意：**

   - 在使用jieba分词自定义词典时，尽量补充完整信息（词语、词频、词性）。如果缺少词性，则最终且分词的词性标注为"x"。

3. 



## 6.命名实体识别

### 6.1 简介

1. 命名实体识别（Named Entities Recognition, NER），是自然语言处理的一个基础任务，是信息抽取、信息检索、机器翻译、问答系统等NLP技术必不可少的组成部分。

2. **NER研究的命名实体**一般分为3大类（实体类、时间类和数字类）和七小类（人名（PER）、地名（LOC）、组织机构名（ORG）、时间、日期、货币和百分比）。其中人名、地名和组织机构名较为复杂。

3. **NER效果评判**（评判一个命名实体是否被正确识别）有两个方面：
   - 实体的边界是否正确，即确定哪些词属于实体。
   - 实体的类型是否标注正确，即确定实体是属于人名或组织机构名哪一种实体。
   
4. **现状：**NER侧重高召回率，但在信息检索领域，高准确率更重要。

5. **NER难点:**

   - 各类命名实体没有严格的命名规范
   - 中文命名实体没有类似英文那样明确的单词边界及标志
   - 中文分词和命名实体识别相互影响
   - 网络汉语文本实体组成方式更加复杂
   - 现存标注语料老旧、覆盖面低
   - 命名实体歧义消岐困难

6. **中文命名实体的难点：**

   - 各类命名实体的数量众多。根据1998年人民日报语料库统计，人名将近2万个，而这些人民大多属于**未登录词**（Out Of Vocabulary, OOV，不在词典中的词，如人名、地名、机构名、一些新词等等）。
   - 命名实体的构成规律复杂。中文人民分中国人民、日本人名等；机构名种类也繁多。
   - 嵌套情况复杂。如机构名不仅嵌套了大量的地名，还嵌套了相当数量的机构名。
   - 长度不确定。主要是机构名长度变化极大，两个字到几十个字范围的都有。  

7. **NER作用：**

   命名实体识别是信息提取、问答系统、语法分析、机器翻译等应用的重要工具。

8. 

### 6.2 NER方法

1. 基于规则的命名实体识别
   - 利用手工构造规则模板，选用特征包括统计信息、标点符号、关键字、位置词（尾词）、中心词等方法，结合命名实体库，对每条规则进行权重赋值，然后通过实体与规则的相符情况来进行类型判断。即：将文本与规则进行匹配来进行命名实体识别。
   - 特点：规则往往依赖于具体语言、领域和文本风格，可移植性差、更新维护困难。
2. 基于统计的命名实体识别
   - **主要思想**：是基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决。利用语料来学习标注模型，从而对句子的各个位置进行标注。
   - **主要方法**：隐马尔科夫模型（HMM）、最大熵模型（ME）、支持向量机（SVM）、条件随机场（CRF）。
   - **制约问题**：该方法对语料库的依赖比较大。
   - **HMM和CRF比较**：
     1. 相同点：都可以用来解决序列标注问题
     2. 不同点1：CRF是在给定观察的标记序列下，计算整个标记序列的联合概率；而HMM是在给定当前状态下，定义下一个状态的分布。
     3. 不同点2： CRF能够捕捉全局的信息，并能够进行灵活的特征设计，效果要比HMM好不少。
     4. 不同点3：  参考（ https://blog.csdn.net/qq_42851418/article/details/83269545 ）![img](https://img-blog.csdn.net/2018102309500466?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyODUxNDE4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 
     5. 
   - 
3. 基于混合方法（规则+统计）的命名实体识别
   - 目前的主流方法是序列标注方式，即特征模板+CRF。
4.  基于深度学习方法的命名实体识别
   - Bi-LSTM
   - Bi-LSTM+CRF
5. 

### 6.3 NER实战

1. **nltk.chunk.ne_chunk(tagged)**

   代码示例:

   ```python
   import nltk
   text = '钟南山院士奋战在疫情最严重的前线-武汉'
   tokens = nltk.word_tokenize(text)  #分词  
   tagged = nltk.pos_tag(tokens)  #词性标注  
   entities = nltk.chunk.ne_chunk(tagged)  #命名实体识别
   # 为了方便查看，把命名实体识别的结果以树结构的形式绘制出来
   from nltk.corpus import treebank
   t = treebank.parsed_sents('wsj_0001.mrg')[0]
   t.draw()
   ```

2. **用 keras 实现NER：**参考（ https://blog.csdn.net/hufei_neo/article/details/90741353 ）

   - 命名实体识别采用的方法：Bi-LSTM+CRF
   - 

3. **用pytorch实现NER：**参考（https://blog.csdn.net/MaggicalQ/article/details/88980534）

   -  实现不同的模型（包括**HMM**，**CRF**，**Bi-LSTM**，**Bi-LSTM+CRF**）来解决中文命名实体识别问题 。

4. 

## 7. 关键词提取