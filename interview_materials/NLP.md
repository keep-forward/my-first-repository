# NLP 面试题

## 1. 什么是NLP？

​        自然语言处理（Natural Language Processing）是人工智能（AI）的一个子领域（AI技术包括大数据、计算机视觉、语音识别、自然语言处理和机器学习五大部分）。

​        NLP是研究人与人交互中以及在人与计算机交互中的语言问题的一门科学。为了建设和完善语言模型，NLP建立计算框架，提出相应的方法来不断的完善设计各种实用系统，并探讨这些实用系统的评测方法。

## 2. NLP的主要十大研究方向

### （1）信息抽取

​        从给定文本中抽取重要信息，比如时间、地点、任务、时间、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因对谁、做了什么事、有什么后果。

### （2）文本生成

​        机器像人一样使用自然语言进行表达和写作。依据输入的不同，文本生成技术主演包括数据到文本生成和文本到文本生成。

​        数据到文本生成是指将包含键值对的数据转化为自然语言文本；文本到文本生成是对输入文本进行转化和处理从而产生新的文本。

### （3）问答系统

​        对于一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别、形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳答案。

### （4）对话系统

​        系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。

### （5）文本挖掘

​        包括文本聚类、分类、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。

### （6）语音识别和生成

​        语音识别是将输入计算机的语音符号识别转换成书面语表示。语音生成又称文语转换、语音合成，它是指将书面文本自动转换成对应的语音表征。

### （7）信息过滤

​        通过计算机系统自动识别和过滤符合特定条件的文档信息。通常指网络有害信息的自动识别和过滤，主要用于信息安全和防护，网络内容管理等。

### （8）舆情分析

​        指收集和处理海量信息，自动化地对网络舆情进行分析，以实现及时应对网络舆情的目的。

### （9）信息检索

​        对大规模的文档进行索引。

​        可简单对文档中的词汇，赋之以不同的权重来建立索引，也可建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的稳定。

### （10）机器翻译

​        把输入的源语言文本通过自动翻译获得另外一种语言的文本。机器翻译从最早的基于规则的方法到20年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。

## 3. 处理NLP任务的一般步骤

如下图总结：

![](https://upload-images.jianshu.io/upload_images/1667471-37315f7baaee75f4.jpg)

一般流程按照上图列举明细如下：

### （1）获取语料

1. #### 语料定义：

   是NLP任务研究的内容。通常用一个文本集合作为语料库（Corpus）

2. #### 来源：

   a. 已有语料（积累的文档）

   b. 下载语料（搜狗语料、人民日报语料）

   c. 抓取语料

### （2）语料预处理

1. ####  语料清晰

   - 解释：留下有用的，删除噪音数据
   - 常见的数据清晰方式：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。

2. ####  分词

   - 解释：将文本分成词语
   - 常见的分词算法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法

3. ####  词性标注

   - 解释：给词语打词类标签，如形容词、动词、名词等，这在情感分析和知识推理等任务中需要
   - 常见的词性标注方法：基于规则和基于统计（如基于最大熵的词性标注、基于统计最大概率输出词性标注和基于HMM的词性标注）

4. ####  去停用词

   去掉对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等。

### （3）特征工程

### （4）特征选择

### （5）模型训练

### （6）评价指标

### （7）模型上线应用

## 4. 中文分词技术

### 4.1 简介

1.  词是最小的能够独立活动的有意义的语言成分。
2. 

### 4.2 分词方法

1. **基于规则分词**

   - 正向最大匹配法（Maximum Match Method, MM法）

   - 逆向最大匹配法（Inverse Maximum Match Method，IMM法）

     由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精度。所以IMM比MM的误差要小。

   - 双向最大匹配法

     双向最大匹配规则：

     1. 如果正反分词结果次数不同，则取分词数量较少的那个（如“南京市/长江/大桥”的分词数量为3，而“南京市/长江大桥”的分词数量为2，所以返回分词数量为2的）
     2. 如果分词结果次数相同：
        - 分词结果相同，就说明没有歧义，可返回任意一个。
        - 分词结果不同，返回其中单字较少的那个。如对“研究生命的起源”进行分词，MM法分词结果为“['研究生---','命---','的---','起源---',]”，其中单字个数为2个；而IMM法分词结果为“['研究---','生命---','的---','起源---']”，其中单字个数为1个。所以返回IMM的分词结果。

   - 基于规则分词的优缺点：

     1. 优点：简单高效
     2. 缺点：词典维护是一个很庞大的工程，网络新词也是层出不穷，很难覆盖到所有词。

   - 

     

2. **基于统计分词**

   2.1 语言模型：n-gram模型，拉普拉斯平滑算法，解决分子分母为0的问题。

   2.2 分词算法：HMM和CRF等。以可采用深度学习网络如CNN、LSTM来发现一些模式和特征。

   - HMM分词是字作为字符串中的序列标注任务来实现的。其基本思路是：每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位），现规定每个字最多只有四个词位：即B（词首）、M（词中）、E（词尾）、S（单独成词）。

   

3. **基于规则+统计分词**

   实际工程应用中，最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。

   

### 4.3 分词工具jieba

1. 简介

   是先基于词典的方式进行分词，然后再用统计分词方法进行辅助的实现。功能：分词，以及分词之上的关键词提取和词性标注等。

2. 分词模式

   - 精确模式：将句子最精确的切开，适合文本分词。默认模式。jieba.cut(sent, cut_all=False)或者jieba.cut(sent)
   - 全模式：将句子中所有可以成词的词语都扫描出来，速度快，但不能解决歧义。jieba.cut(sent, cut_all=True)
   - 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适用于搜索引擎分词。jieba.cut_for_search(sent)
   - 

3. jieba常用方法

   - **添加自定义词典:**

     ```python
     '''
     两类：载入词典和调整词典
     '''
     (1)载入词典
     # 加载系统词典
     jieba.set_dictionary('D:/python_projects/NLP_Learning_data/chineseCutWord/data/dict.txt.big')
     # 加载用户自定义词典
     jieba.load_userdict('D:/python_projects/NLP_Learning_data/chineseCutWord/data/user_dict.utf8')
     (2)调整词典
     # 调整词典,向词典中添加一个词。freq 和 tag 可以省略，freq 默认为一个计算值
     jieba.add_word(word, freq=None, tag=None)
     # 在词典中删除一个词
     jieba.del_word(word)
     '''
     使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其（或不能）被分出来。注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。
     示例：jieba.suggest_freq(('证件照片'), tune=True)
     '''
     jieba.suggest_freq(segment, tune=True)
     
     ```

   - **词典格式：**

     每一行为三个部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。该词典文件需为utf-8编码。

   - **cut()和lcut()：**

     ```python
     '''
     此方法接受三个参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型;
     lcut 方法直接返回 list，cut 方法返回一个 可迭代的 generator
     '''
     sent = '中文分词是文本处理不可或缺的一步！'
     print('cut is:', jieba.cut(sent))
     结果为：cut is: <generator object Tokenizer.cut at 0x02589530>
     print('精确模式：', '/'.join(jieba.cut(sent)))
     结果为：精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！
     print('lcut is:', jieba.lcut(sent))
     结果为：lcut is: ['中文', '分词', '是', '文本处理', '不可或缺', '的', '一步', '！']
     ```

     

   - **jieba.analyse:**关键词提取

     ```python
     (1). 基于 TF-IDF（term frequency–inverse document frequency） 算法的关键词抽取。
     # 1. 新建TFIDF实例，idf_path为IDF频率文件，可不填
     jieba.analyse.TFIDF(idf_path=None) 
     # 2. 关键词提取所使用的的逆向文件频率（即IDF）文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_idf_path(filename)
     # 3. 关键词提取所用到的停用词文本语料库可以切换成自定义语料库的路径，filename为自定义语料库的路径
     jieba.analyse.set_stop_words(filename)
     # 4. 开始提取关键字
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回几个 TF/IDF 权重最大的关键词，默认值为 20
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，默认为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
     
     
     (2).是基于 TextRank 算法的关键词抽取。
     # 1. 新建TextRank实例
     jieba.analyse.TextRank()
     # 2. 提取关键词
     '''
     第一个参数：待提取关键词的文本
     第二个参数：返回关键词的数量，重要性从高到低排序
     第三个参数：是否同时返回每个关键词的权重
     第四个参数：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词
     '''
     keywords =jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’))
     
     # 打印结果：
     for item in keywords:
         # 分别为关键词和相应的权重
         print(item[0], item[1])
     或者
     for w,t in keywords:
         print('%s, %s'%(w, t))
     ```

   - 

4. 

### 4.4 nltk（Natural Language Toolkit）分词

1.  **简介：**

   nltk是一个高效的python构建平台，用来处理自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如wordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库。

2. **说明：**

   - nltk本身自带了大量的语料，但**大部分是英文的**。

   - nltk自带了很多统计的功能，但针对中文来讲，**分词的工作需要我们手动来完成**。然后再把处理过的文本封装成nltk的“text”对象，之后才能使用nltk进行处理。

     代码举例：

     ```python
     import jieba
     import nltk
     import re
     word="盗墓不是请客吃饭，不是做文章，不是绘画绣花，不能那样雅致，那样从容不迫，文质彬彬，那样温良恭俭让。"
     # 去除标点符号
     cleaned_data = ''.join(re.findall(r'[\u4e00-\u9fa5]', word))
     # lcut 方法直接返回 list
     wordlist = jieba.lcut(cleaned_data)
     # 封装成nltk的“text”对象
     text = nltk.Text(wordlist)
     print(text)
     # 查看单词的上下文
     text.concordance(word='僵尸', width=20, lines=10)
     # 搜索共同上下文
     text.common_contexts(['僵尸', '鬼'])
     # 统计词频
     text.count(word='胖子')
     # 绘制离散图
     words = ['鬼', '僵尸', '胖子', '老胡']
     text.dispersion_plot(words)
     ```

   - **nltk分词基本针对英文的，中文分词主要使用jieba分词**。

   - **下载语料、预训练模型等**（使用前需要首先执行以下代码）。 除了一些个人数据包还可以下载整个集合（使用“all”），或者仅下载书中例子和练习中使用到的数据（使用“book”），或者仅下载没有语法和训练模型的语料库（使用“all-corpora”） .

     ```python
     import nltk
     nltk.download()
     ```

   - 

3. **代码实现：**

   ```python
   import nltk
   import re
   english='H:\\自然语言处理\\Experiment2\\English.txt'
   with open(english,'r',encoding='utf-8') as file:    
       u=file.read()
   str=re.sub('[^\w ]','',u)
   # 分词
   print(nltk.word_tokenize(str))
   # 对分完词的结果进行词性标注
   print(nltk.pos_tag(nltk.word_tokenize(str)))
   ```

4. 



## 5. 词性标注

### 5.1 简介

1. 词性标注是确定给定句子中的每个词的词性（名词n、动词v、形容词a、副词d、助词u、代词等）并加以标注的过程。
2. 

### 5.2 词性标注规范

1. 中文领域中尚无统一的标注标准。较为主流的是北大词性标注集合宾州词性标注集。

### 5.3 词性标注方法

1. **方法：**

   - **jieba.posseg**

     示例：

     ```python
     import jieba.posseg as psg
     sent = '中文分词是文本处理不可或缺的一步！'
     seg_list = psg.cut(sent)
     print(' '.join(['{0}/{1}'.format(w,t) for w,t in seg_list]))
     结果：
     中文/nz 分词/n 是/v 文本处理/n 不可或缺/l 的/uj 一步/m ！/x
     ```

   - **HMM进行词性标注**

     示例：（待完成）

   - **nltk词性标注：**先分词再词性标注

     ```python
     import nltk
     import re
     english='H:\\自然语言处理\\Experiment2\\English.txt'
     with open(english,'r',encoding='utf-8') as file:
         u=file.read()
     str=re.sub('[^\w ]','',u)
     # 分词
     print(nltk.word_tokenize(str))
     # 对分完词的结果进行词性标注
     print(nltk.pos_tag(nltk.word_tokenize(str)))
     ```

   - 

2. **注意：**

   - 在使用jieba分词自定义词典时，尽量补充完整信息（词语、词频、词性）。如果缺少词性，则最终且分词的词性标注为"x"。

3. 



## 6.命名实体识别

### 6.1 简介

1. 命名实体识别（Named Entities Recognition, NER），是自然语言处理的一个基础任务，是信息抽取、信息检索、机器翻译、问答系统等NLP技术必不可少的组成部分。

2. **NER研究的命名实体**一般分为3大类（实体类、时间类和数字类）和七小类（人名（PER）、地名（LOC）、组织机构名（ORG）、时间、日期、货币和百分比）。其中人名、地名和组织机构名较为复杂。

3. **NER效果评判**（评判一个命名实体是否被正确识别）有两个方面：
   - 实体的边界是否正确，即确定哪些词属于实体。
   - 实体的类型是否标注正确，即确定实体是属于人名或组织机构名哪一种实体。
   
4. **现状：**NER侧重高召回率，但在信息检索领域，高准确率更重要。

5. **NER难点:**

   - 各类命名实体没有严格的命名规范
   - 中文命名实体没有类似英文那样明确的单词边界及标志
   - 中文分词和命名实体识别相互影响
   - 网络汉语文本实体组成方式更加复杂
   - 现存标注语料老旧、覆盖面低
   - 命名实体歧义消岐困难

6. **中文命名实体的难点：**

   - 各类命名实体的数量众多。根据1998年人民日报语料库统计，人名将近2万个，而这些人民大多属于**未登录词**（Out Of Vocabulary, OOV，不在词典中的词，如人名、地名、机构名、一些新词等等）。
   - 命名实体的构成规律复杂。中文人民分中国人民、日本人名等；机构名种类也繁多。
   - 嵌套情况复杂。如机构名不仅嵌套了大量的地名，还嵌套了相当数量的机构名。
   - 长度不确定。主要是机构名长度变化极大，两个字到几十个字范围的都有。  

7. **NER作用：**

   命名实体识别是信息提取、问答系统、语法分析、机器翻译等应用的重要工具。

8. 

### 6.2 NER方法

1. 基于规则的命名实体识别
   - 利用手工构造规则模板，选用特征包括统计信息、标点符号、关键字、位置词（尾词）、中心词等方法，结合命名实体库，对每条规则进行权重赋值，然后通过实体与规则的相符情况来进行类型判断。即：将文本与规则进行匹配来进行命名实体识别。
   - 特点：规则往往依赖于具体语言、领域和文本风格，可移植性差、更新维护困难。
2. 基于统计的命名实体识别
   - **主要思想**：是基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决。利用语料来学习标注模型，从而对句子的各个位置进行标注。
   - **主要方法**：隐马尔科夫模型（HMM）、最大熵模型（ME）、支持向量机（SVM）、条件随机场（CRF）。
   - **制约问题**：该方法对语料库的依赖比较大。
   - **HMM和CRF比较**：
     1. 相同点：都可以用来解决序列标注问题
     2. 不同点1：CRF是在给定观察的标记序列下，计算整个标记序列的联合概率；而HMM是在给定当前状态下，定义下一个状态的分布。
     3. 不同点2： CRF能够捕捉全局的信息，并能够进行灵活的特征设计，效果要比HMM好不少。
     4. 不同点3：  参考（ https://blog.csdn.net/qq_42851418/article/details/83269545 ）![img](https://img-blog.csdn.net/2018102309500466?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyODUxNDE4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 
     5. 
   - 
3. 基于混合方法（规则+统计）的命名实体识别
   - 目前的主流方法是序列标注方式，即特征模板+CRF。
4.  基于深度学习方法的命名实体识别
   - Bi-LSTM
   - Bi-LSTM+CRF
5. 

### 6.3 NER实战

1. **nltk.chunk.ne_chunk(tagged)**

   代码示例:

   ```python
   import nltk
   text = '钟南山院士奋战在疫情最严重的前线-武汉'
   tokens = nltk.word_tokenize(text)  #分词  
   tagged = nltk.pos_tag(tokens)  #词性标注  
   entities = nltk.chunk.ne_chunk(tagged)  #命名实体识别
   # 为了方便查看，把命名实体识别的结果以树结构的形式绘制出来
   from nltk.corpus import treebank
   t = treebank.parsed_sents('wsj_0001.mrg')[0]
   t.draw()
   ```

2. **用 keras 实现NER：**参考（ https://blog.csdn.net/hufei_neo/article/details/90741353 ）

   - 命名实体识别采用的方法：Bi-LSTM+CRF
   - 

3. **用pytorch实现NER：**参考（https://blog.csdn.net/MaggicalQ/article/details/88980534）

   -  实现不同的模型（包括**HMM**，**CRF**，**Bi-LSTM**，**Bi-LSTM+CRF**）来解决中文命名实体识别问题 。

4. 

## 7. 关键词提取

### 7.1 简介

1. 关键词是代表文章重要内容的一组词。对文本聚类、分类、自动摘要等起重要的作用。

### 7.2 关键词提取常用方法

1.  **有监督的关键词提取方法**

   - 方法概述：

     将关键词提取视为分类问题，通过**构建一个较为丰富和完善的词表**，然后通过判断每个文档与词表中每个词的匹配程度，以类似**打标签**的方式（候选词的标签：关键词和非关键词），达到关键词提取的效果。

   - 优点：

     能够获取较高的精度。

   - 缺点：

     （1）由于需要大量的标注数据，人工成本过高；

     （2）每天都会有大量的新信息出现，人工维护这个词表需要很高的人力成本。

   - 

2. **无监督的关键词提取方法**

   - 方法概述：

     先抽取出文章的候选词（即分词、去除干扰词等），然后利用某些方法对各个候选词进行打分，然后输出topK个分值最高的候选词作为关键词。

   - 方法分类：

     主要有三类：

     1. **基于统计特征的关键词提取**：如TF-IDF方法

        基本思想是：利用文档中词语的统计信息抽取文档的关键词。

     2. **基于词图模型的关键词提取**：如PageRank、TextRank方法等

        基本思想是：首先构建文档的语言网络图，然后对语言进行网络图分析，	在这个图上寻找具有重要作用的词或者短语，这些词或者短语就是文档的关键词。

        TextRank方法**重要特征：可以脱离语料库，仅对单篇文档进行分析。**

     3. **基于主题模型的关键词提取**：如LSA（Latent Segmantic Analysis，潜在语义分析）、LDA（Latent Dirichlet Allocation，隐含狄里克雷分布）等

        基本思想：主要利用主题模型中文档对主题的分布和主题对词的分布来进行关键词提取的。

     4. 

   - 优点：

     不需要人工生成、维护的词表，也不需要人工标准语料辅助进行训练，更加快捷。

   - 缺点：

     无法有效综合利用多种信息对候选关键词进行排序，所以无法与有监督方法媲美。

   - 

3. 

### 7.3 关键词提取常用工具包

- jieba

  简介：分词工具，另提供关键词提取、词性标注等功能。jieba.analyse可用于关键词提取。

- Gensim

  简介：是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持TF-IDF、LSA、LDA和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口。

- textrank4zh（TextRank算法工具）

  简介： TextRank4ZH是针对中文文本的TextRank算法的python算法实现。 参考： https://github.com/letiantian/TextRank4ZH 

- SnowNLP（中文分析）简体中文文本处理

  简介： SnowNLP是一个python写的类库，可以方便的处理中文文本内容。 且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。参考  https://pypi.org/project/snownlp/ 

- TextBlob（英文分析）

  简介： https://pypi.org/project/textblob/ 

参考： https://blog.csdn.net/asialee_bird/article/details/96454544 

### 7.4 常用关键词提取方法介绍

整体参考： https://blog.csdn.net/asialee_bird/article/details/96454544 

1. TF-IDF算法

   - 方法介绍：TF-IDF（Term Frequency-Inverse Document Frequency, 词频-逆文件频率），是一种用于信息检索和文本挖掘的常用加权技术。

     是一种统计方法，用于评估一个文档集中一个词对某份文档的重要程度。

   -  主要思想：如果一个词在一个文档中出现的频率越高，并且在其他文档中很少出现，则认为这个词对文档的区分能力就越强。

   - 方法详解：

     词频TF，表示一个词在一片文档中出现的频率，这个数字通常会被归一化（一般是词频除以文档总词数），以防止它偏向长的文档。tf(word) = (word在文档中出现的次数)/(文档总词数)。

     逆向文件频率IDF，统计一个词在文档集的多少个文档中出现。如果包含词t的文档越少，IDF越大，说明词t具有很好的类别区分能力。idf=log(语料库即文档集中文档总数/(包含词t的文档数+1))。分母加1是采用拉普拉斯平滑，避免有部分新词没有在语料库中出现过而导致分母为零的情况出现，增强算法的健壮性。

     TF-IDF：TF*IDF，某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。

   - 方法特点：该方法容易实现，但其结构简单，并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。               

2. TextRank算法

   - **重要特点：**该算法可以脱离语料库的背景，仅对单篇文档进行分析就可以提取该文档的关键词。

3. LSA主题模型算法

   - 主要采用SVD（Singular Value Decomposition，奇异值分解）的方法进行暴力破解。

4. LDA主题模型算法

   - 主要通过贝叶斯学派的方法对分布信息进行拟合。

   - 简介：

     主题模型是一种统计模型用于发现文档集合中出现的抽象“主题”。主题建模是一种常用的文本挖掘工具，用于在文本体中发现隐藏的语义结构。

     LDA算法假设文档中主题的先验分布和主题中词的先验分布都服从狄里克雷分布。该方法的理论基础是贝叶斯理论。LDA也称三层贝叶斯概率模型，包括词、主题和文档三层结构；利用文档中单词的共现关系对单词按主题聚类，得到“文档=主题”和“主题-单词”两个概率分布。

   - LDA的训练是根据现有的数据集生成文档-主题分布矩阵和主题-词分布矩阵，Gensim中有实现好的训练方法，直接调用即可。

5. word2vec词聚类算法

   - 主要思路：对于用词向量表示（ **Word2Vec** 模型）的词语，通过K-means算法对文章中的词进行聚类，选择聚类中心作为文本的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topK个距离聚类中心最近的词作为关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。

6. 信息增益关键词提取法

7. 互信息关键词提取法

8. 卡方校验关键词提取法

9. 基于树模型的关键词提取算法

10. 

### 7.5 关键词提取方法总结

- 分类：

  空间向量模型：（Space Vector Model，SVM），如TF-IDF，TextRank算法

  主题模型：如LSA、LDA

- 比较：

  空间向量模型：只能基于文档本身提取关键词，而不能利用语义信息；

  而主题模型：可以提取文档中隐含的主题信息。LSA可以通过SVD将词、文档可以映射到低维的空间，不仅可以利用文本语义信息，而且大大降低计算的代价，提高分析质量。

  LSA缺点：SVD计算复杂度高，特征空间维度较大，计算效率低下；且对新的一个文档需要对整个空间重新训练才能得到加入新文档后对应的分布信息。另LSA还存在对词的频率分布不敏感、物理解释性薄弱等问题。

- 

### 7.5 关键词提取的步骤

- 算法训练阶段步骤：
  1. 加载已有的文档数据集
  2. 加载停用词表
  3. 对数据集中的文档进行分词
  4. 根据停用词表，过滤干扰词
  5. 视情况而定，使用词性对词进一步筛选，只要名词性的词语，其他词性的词语视为干扰词过滤掉
  6. 根据数据集训练算法
- 对新文档进行关键词提取步骤：
  1. 对新文档进行分词
  2. 根据停用词表，过滤干扰词
  3. 根据训练好的算法提取关键词
- 

## 8 句法分析（parsing）

### 8.1 简介

- 句法分析是自然语言处理的核心技术，是对语言进行深层次理解的基石。**语义分析通常就是以句法分析的输出结果作为输入以便获得更多的指示信息。**

- **常见应用**：计算机的翻译、文字的注释、一对一的问答系统、信息的自然摘录以及自动搜索等。

- **主要任务**：识别出句子所包含的句法成分以及这些成分之间的关系，一般以**句法树**来表示句法分析的结果。

- **分类**：**句法结构分析**（获取整个句子的句法结构或完全短语结构）和**依存关系分析**（以获取局部成分为目的的句法分析）。

   ![img](https://img-blog.csdnimg.cn/20181223155910994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70) 

- **难点**：

  1. **语句歧义**

     语言文化博大精深，很多词都有多种意思。

  2. **搜索空间（汉语的搜索量太大）**

     句法分析是一个极为复杂的任务，候选树个数随句子增多呈指数级增长，搜索空间巨大。

  3. 

- 

### 8.2 句法分析的数据集与评测方法

- **数据集**

  句法分析的数据集复杂的多，其是一种树形的标注结构，因此又称树库。如下图：

   ![这里写图片描述](https://img-blog.csdn.net/20180128154100453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjgwMzE1MjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

  英文树库：使用最多的是美国宾夕法尼亚大学加工的英文宾州树库（PTB，Penn TreeBank）

  中文树库：中文宾州树库（CTB，Chinese TreeBank，目前使用最多）、清华树库（Tsinghua Chinese TreeBank）、台湾中研院树库。

  **注意**：不同树库有不同的标记体系，使用时**切忌**使用一种树库的句法分析器（Parser），然后用其他树库的标记体系来解释。

- **评测方法**

  句法分析评测的**主要任务**：评测句法分析器生成的树结构与手工标注的树结构之间的相似度。

  主要考虑两方面的**性能**：满意度（测试句法分析器是否适合或胜任某个特定的NLP任务）和效率（主要用于对比句法分析器的运行时间）。

  **主流评测方法**：PARSEVAL评测体系（粒度比较适中、较为理想的评价方法），主要指标有准确率、召回率、交叉括号数。

- 

### 8.3 句法分析的常用方法

- **分类：**

  1. 分为基于规则的分析方法和基于统计的分析方法

     - 基于规则的分析方法

       **基本思路**：由人工组织语法规则，建立语法知识库，通过条件约束和检查来实现句法结构歧义的消除。

     - 基于统计的分析方法

       包括生成式依存分析法、判别式依存分析法和确定性依存分析法。

     - 基于深度学习的方法

       主要研究工作集中在特征表示方面。传统方法的特征表示主要采用人工定义原子特征和特征的组合，而深度学习则把原子特征（词、词性、类别标签）进行向量化，在利用多层神经元网络提取特征。

     - 

  2. 根据句法分析树的形成方向分为三种类型：自顶向下的分析方法、自底向上的分析方法、两者相结合的分析方法

     - 自顶向下的分析方法：

       分析树从根节点开始不断生长，最后形成分析句子的叶结点。

     - 自底向上的分析方法：

       从句子符号串开始，执行不断规约的过程，最后形成根节点。

       如基于移进-规约的句法分析模型，就是一种自下而上（自底向上）的方法。

     - 两者相结合的分析方法：

  3. **句法结构分析**分为完全语法分析和浅层语法分析（局部语法分析），如8.1分类图。

     - 完全语法分析

       需确定句子所包含的全部句法信息，并确定句子中各成分之间的关系。

     - 浅层语法分析（局部语法分析）

       只要求识别句子中某些结构相对简单的独立成分，例如非递归的名词短语、动词短语等，这些被识别出来的结构通常称为语块（chunk）。

       **两个主要子任务**：语块的识别和分析（主要任务）、语块之间的依附关系分析。

  4. 

- **句法分析的常用方法**

  参考书中P110-P114部分。

  1. **基于PCFG的句法分析**

     PCFG（Probabilistic Context Free Grammar，概率上下文无法法），是基于概率的短语结构分析方法。是目前最成功的基于语法驱动的统计句法分析方法。

     **PCFG的三个基本问题**：

     - 给定上下文无关文法G，如何计算句子S的概率，即计算P(S/G)

       解决方法：使用内向算法和外向算法

     - 给定上下文无关文法G以及句子S，如何选择最佳的句法树，即计算arg max P(T/S,G)

       解决方法：使用Viterbi算法

     - 如何为文法规则选择参数，使得训练句子的概率最大，即计算arg max P(S/G)

       解决方法：使用EM算法

  2. **基于最大间隔马尔科夫网络的句法分析**

     最大间隔是SVM（支持向量机）中的重要理论，而马尔科夫网络是概率图模型中一种具备一定结构处理关系能力的算法。最大间隔马尔科夫网路（Max-Margin Markov Networks）就是这两者的结合，能够解决复杂的结构化预测问题，尤其适用于句法分析任务。**是一种判别式的句法分析方法**。

  3. **基于CRF的句法分析**

     将句法分析作为序列标注问题来解决，可利用CRF模型。与PCFG模型相比，采用CRF模型进行句法分析的主要不同点在于概率计算方法和概率归一化的方式。**CRF模型最大化的是句法树的条件概率值而不是联合概率值，并且对概率进行归一化。也是一种判别式的方法，需要融合大量的特征。**

  4. **基于移进-规约的句法分析模型**

     是一种自下而上的方法。从输入串开始，逐步进行“归约”，直到归约到文法的开始符号（S）。**其操作的基本数据结构是堆栈。**

  5. 

- 

### 8.4 句法分析的常用工具

- nltk

   由宾夕法尼亚大学计算机和信息科学使用python语言实现的一种自然语言工具包，其收集的大量公开数据集、模型上提供了全面、易用的接口，涵盖了分词、词性标注(Part-Of-Speech tag, POS-tag)、命名实体识别(Named Entity Recognition, NER)、句法分析(Syntactic Parse)等各项 NLP 领域的功能。 

  ```python
  from nltk.parse import stanford
  ```

- Stanford NLP

   由斯坦福大学的 NLP 小组开源的 Java 实现的 NLP 工具包，同样对 NLP 领域的各个问题提供了解决办法。

  注意：

  1. Stanford NLP工具包需要Java 8及之后的版本； 
  2. 配置nltk，import nltk, nltk.download()下载NLTK数据包，放在./Anaconda3/nltk_data，并将此路径配置在环境变量的path中

- Stanford CoreNLP

  安装：pip install stanfordcorenlp（参考 https://blog.csdn.net/m0_37306360/article/details/84712213 ）

  国内源安装：pip install stanfordcorenlp -i https://pypi.tuna.tsinghua.edu.cn/simple 

  

  ```python
  from stanfordcorenlp import StanfordCoreNLP
  ```

  

- HanLP

  HanLP是一系列模型与算法组成的NLP工具包。提供了中文依存句法分析功能。

  Github地址：https://github.com/hankcs/pyhanlp

  官网：http://hanlp.linrunsoft.com/

  \# 安装：pip install pyhanlp

  \# 国内源安装：pip install pyhanlp -i https://pypi.tuna.tsinghua.edu.cn/simple

  ```python
  from pyhanlp import *
  ```

- 

### 8.5 句法分析现状

- 句法分析算法实际性能离真正实用化还有不小的距离，主要原因在于，在语言学理论和实际的自然语言应用之间存在着巨大的差距。

## 9. 文本向量化

### 9.1 简介

- 文本表示是自然语言处理中的基础工作，文本表示的好坏直接影响到整个自然语言处理系统的性能。
- 文本向量化是文本表示的一种重要方式。

### 9.2 文本表示方法：

**文本表示一般称为词嵌入**（Word Embedding）方法，即把文本中的词嵌入到文本空间中，用一个向量来表示它。**分为离散表示和分布式表示**：参考（ https://zhuanlan.zhihu.com/p/42310942 ）

1. **离散表示**

   1.1 **离散表示的代表就是词袋模型**（Bag of Word），One-hot（也叫独热编码）、TF-IDF、n-gram都可以看做是词袋模型。

   - One-hot
   - Bi-gram和N-gram
   - TF-IDF
   - 

   1.2 **词袋模型的缺点**

   - 维度灾难
   - 无法保留语义信息。词袋模型只是将词符号化，是不包含任何语义信息（包括词序）的
   - 矩阵稀疏

2. **分布式表示**

   分布式表示的经典技术就是word2vec技术。

   - 共现矩阵

     词文档的共现矩阵主要用于发现主题(topic)，用于主题模型，如LSA。

     局域窗中的word-word共现矩阵可以挖掘语法和语义信息。

   - 神经网络语言模型（NNLM，Nerual Network Language Model）

     构建一个语言概率模型。

   - C&W模型

     以生成词向量为目标。与NNLM模型的目标词在输出层不同，C&W模型的输入层就包含了目标词，其输出层也变为一个节点，该节点输出值的大小代表n元短语的打分高低。

   - CBOW

     目标是根据上下文的词语来预测当前词语的概率。

   - Skip-gram

     目标是根据当前词语来预测上下文的概率（前后可能出现的词语）。

   - fasttext模型

     

3. 

### 9.3 word2vec技术

- 简介

  word2vec技术，不是模型，而是为了利用神经网络从大量无标注的文本中提取有用信息而产生的一种技术（思想），是**无监督的**（不需要大量的人工标记样本就可以得到质量不错的词嵌入向量）。

  它包含**两种模型和两种加速训练方法**：

  - 两种模型：

    1. CBOW模型（Continuous Bag of-Words）

       目标是根据上下文的词语来预测当前词语的概率。

    2. Skip-gram

       目标是根据当前词语来预测上下文的概率（前后可能出现的词语）。

  - 两种方法（这两种技巧使得原本参数繁多、计算量巨大的神经网络语言模型变得容易计算）：

    1. 层次softmax

       通过构建一种有效的树结构（哈夫曼树，Huffman tree）来加速计算词语的概率分布的方法。

    2. 负例采样（Negative Sampling）

       通过随机抽样负样本，与正样本一起参加每次迭代，变成一个二分类问题，从而减少计算量。

  - 

- 引申

  word2vec的基础就是**语言模型**。语言模型说白了就是用来计算某一个句子出现的概率。对于一个语法和语义都合理的句子，一个训练好的语言模型会计算出一个非常高的概率；相反，计算出来的概率就非常低。

- word2vec技术的优缺点

  - 优点

    很好地提取词语的语义信息，可以用来计算**词语间的相似度**。

  - 缺点

    1. 关键词提取算法准确率不高，丢失了很多关键信息
    2. 用word2vec技术计算**句子或者其他长文本间的相似度时，丢失了文本中的语序信息**。而文本的语序也包含重要信息。如“小王送给小红一个苹果”和“小红送给小王一个苹果”虽然组成两个句子的词语相同，但是表达的意思却完全不同。

### 9.4 doc2vec/str2vec技术

- 简介

  针对上节中word2vec的缺点导致的问题，谷歌工程师在word2vec的基础上进行了扩展，提出了doc2vec。

- 两种模型

  DM（Distributed Memory）和DBOW（Distributed Bag of Words）对应着word2vec技术中的CBOW和Skip-gram模型。

  1. DM

     与CBOW模型类似，DM模型试图预测给定上下文中某单词出现的概率，只不过**DM模型的上下文不仅包括上下文单词而且还包括相应的段落（即增加了一个与词向量长度相等的段向量）。**

  2. DBOW

     与Skip-gram模型值给定一个词语预测目标词概率分布类似，DBOW则在仅给定的段落向量的情况下预测段落中一组随机单词的概率。

  两种模型的比较：

  DM模型通过**词向量和段落向量**预测目标词的概率分布；

  DBOW模型通过**段落向量**预测段落中某个随机抽取的词组的概率分布。

- 特点

  doc2vec不仅提取了文本的语义信息，而且提取了文本的语序信息。

- 

### 9.5 文本向量化实战工具

- gensim模块

  ```python
  from gensim.models import Word2Vec
  from gensim.models.Word2Vec import LineSentence
  ```

  

- 

## 10. 情感分析技术

### 10.1 简介



















































## 知识图谱技术

### 概要：

参考（ https://developer.51cto.com/art/201903/593471.htm ）