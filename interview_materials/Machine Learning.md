

# Machine Learning 面试题



## 1. HMM（Hidden Markov Model,隐马尔科夫模型）

- ### 简介

  HMM模型是可以用于标注问题 的统计学模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。标注问题中，状态对应着标记，是给定观测的序列预测其对应的状态序列。

- 

- ### 使用到HMM问题特征

  1. 问题是基于序列的，如时间序列，状态序列；

  2. 问题中有两类数据：一类序列数据是可以观测到的，即观测序列；另一类数据是不能观测到的，即隐藏状态序列，简称状态序列。

     举例：常用的谷歌输入法等就是采用HMM模型，根据你之前的输入，判断接下来要输入什么给出提示。

- ### 模型定义

  对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有观测状态的集合，即
  $$
  Q=｛q_1,q_2,...,q_n｝,V= ｛v_1,v_2,...,v_m｝
  $$
  其中，N是可能的隐藏状态数，M是所有可能的观察状态数。

  对于一个长度为T的序列，I对应的状态序列，O是对应的观察序列，即
  $$
  I = ｛i_1,i_2,...,i_n｝, O = ｛o_1,o_2,...,o_m｝
  $$
  其中，任意一个隐藏状态it ∈ Q，任意一个观测状态ot ∈ V。

  接着，我们做出以下假设：

  1. #### 齐次马尔科夫链假设

     即任意时刻的隐藏状态只依赖于它前一时刻的隐藏状态。我们得到如果在时刻t的隐藏状态是it=qi，在时刻t+1的隐藏状态是it+1=qj，则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：
     $$
     a_{ij} = P(i_{t+1} = q_j|i_t=q_i)
     $$
     从而得到马尔科夫链的状态转移矩阵A：
     $$
     A = [a_{ij}]_{N*N}
     $$
     
  2. #### 观测独立性假设
  
     即任意时刻的观测状态只仅仅依赖于当前时刻的隐藏状态。如果在时刻t的隐藏状态是it=qj，而对应的观察状态为ot=vk，则该观察状态vk在隐藏状态qj下生成的概率为bj(k)为：
     $$
       b_j(k) = P(o_t = v_k|i_t = q_j)
     $$
       这样bj(k)可以组成观测状态生成的概率矩阵B：
     $$
     B = [b_j(k)]_{N*M}
     $$
     
2.   我们再获得一个初始状态t=1的初始状态概率分布π：
  $$
     π = [π(i)]_N, 其中π(i) = P(i_1 = q_i)
   $$

根据上面的两个假设和初始状态就可以确定一个HMM。

隐马尔科夫模型 λ可以用三元符号表示，即
$$
λ = (A, B, π)
$$
 A，B，π称为隐马尔科夫模型的三要素。

- ### 三个经典问题：

1. #### 评估观测序列概率

   - **问题描述**：即给定模型 λ=(A,B,Π) 和观测序列 O={o1,o2,...oT} ，计算在模型λ下观测序列O出现的概率P(O|λ)

   - **对应的解法**：前向算法（Forward Algorithm）和后向算法（Backward Algorithm）

   - 前向算法实现：

     1. 计算时刻1的各个隐藏状态前向概率：
        $$
        a_1(i) = π(i)*b_i(o_1),  i=1,2,...,N
        $$
   
     
     2. 递推时刻2，3，...，T时刻，也即t=1,2,...,T-1的前向概率
        $$
        a_{t+1} = [Σ_{j=1}^na_t(j)a_{ji}]b_i(o_{t+1}),  i=1,2,...,N
        $$
        
     3. 计算最终结果
        $$
        P(O|λ) = \Sigma_{i=1}^Na_T(i)
        $$
        
     
   - 后向算法实现：
   
     1. 初始化时刻T的各个隐藏状态后向概率：
        $$
        B_T(i) = 1,  i=1,2,...,N
        $$
     
     
     2. 递推时刻T-1，T-2，...，1时刻的后向概率
        $$
        B_t(i) = [Σ_{j=1}^na_{ij}]b_j(o_{t+1})B_{t+1}(i),  i=1,2,...,N
        $$
        
     3. 计算最终结果
        $$
        P(O|λ) = \Sigma_{i=1}^Nπ(i)b_i(o_1)B_1(i)
        $$
        
     
   - 
2. #### 预测问题，也称解码问题

   - **问题描述**：即给定模型 λ=(A,B,Π)和观测序列O={o1,o2,...oT}，求给定观测序列条件下，最可能出现的对应的状态序列。

   - **对应的解法：**基于动态规划的维特比算法。

   - **动态规划原理：**

     最优路径具有这样的特性：如果最优路径在时刻t通过节点it，那么这一路径从结点it到终点iT的部分路径，对于从it到iT的所有可能的部分路径来说，必须是最优的。因为假如不是这样，那么从it到iT就有另一条更好地部分路径存在，如果把它和从i1到it的部分路径连接起来，就会形成一条比原来的路径更优的路径，这是矛盾的。

   - **维特比算法：**依据动态规划原理，我们只需要从时刻t=1开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直至得到时刻t=T状态为i的各条路径的最大概率。时刻t=T的最大概率即为最优路径的概率P，最优路径的终结点iT也同时得到。之后为了找出最优路径的各个结点，从终结点iT开始，由后向前逐步求得结点i(T-1)，...，i1，得到最优路径I=(i1,i2,...,iT)。（详解请参考李航老师的统计学习算法P200-203）。

   - **Veterbi算法实现：**

     

   - 

3. #### 模型参数学习问题

   - **问题描述**：给定观测序列O={o1,o2,...oT}，估计模型 λ=(A,B,Π)的参数，使得该模型下观测序列的条件概率P(O|λ)最大。

   - **对应的解法**：
   
     1. 若训练数据包含观测序列以及对应的状态序列，可由**监督学习算法**实现，该情况下HMM的学习非常简单。
     2. 若训练数据只包含观测序列，可由**非监督学习算法--基于EM算法的Baum-Welch算法**实现。
   
   - **算法实现**：
   
     1. 监督学习算法：
   
        直接利用大数定理的结论“**频率的极限是概率**”，直接给出HMM的参数估计。
   
        假设已给定训练数据包含S个长度相同的观测序列和对应的状态序列
        $$
        ｛(O_1,I_1),(O_2,I_2),...,(O_S,I_S)｝
        $$
        则可以利用极大似然估计法来估计HMM的参数，具体解法如下：
   
        - 转移概率aij的估计
   
          设样本中时刻t处于状态i时刻t+1处于状态j的频数（个数）为Aij，那么状态转移概率aij的估计是：
          $$
          a_{ij} = {A_{ij}\over \Sigma_{j=1}^NA_{ij}}, i=1,2,...,N; j=1,2,...,N
          $$
          
   
        - 观测概率bj(k)的估计
   
          设样本中状态j并观测为k的频数为Bjk，那么状态为j观测为k的概率bj(k)的估计是：
          $$
          b_j(k) = {B_{jk} \over \Sigma_{k=1}^MB_{jk}}, j=1,2,...,N;k=1,2,...,M
          $$
          
   
        - 初始状态概率πi的估计
   
          设样本中状态为i的频数为Si，那么初始状态概率πi的估计是：
          $$
          \pi_i = {Si \over \Sigma_{i=1}^NS_i}, i=1,2,...,N
          $$
          
   
        - 
   
     2. 非监督学习算法-BAUM-Welch算法：
   
        详情参考（ https://www.jianshu.com/p/c450bb061651 或者李航老师统计学习方法中P197-200部分内容）
   
   - 
