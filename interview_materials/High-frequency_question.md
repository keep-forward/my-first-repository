# 高频问题

## 1. 非CS专业（Computer Science,计算机科学）为什么要做机器学习行业？

之前在一本书中有看到过一句话：机器学习产生于控制科学。而我的专业就是控制理论于控制工程，就是控制科学的专业。

## 2. 样本不平衡的解决方法 

**什么是样本不均衡：** 在处理文本分类问题经常会遇到文本不均衡的问题，即训练集中可能会存在某个或者类别下的样本数远大于另一些类别下的样本数目。比如正负样本1:10（类别不平衡比例超过4:1，就会造成偏移） 。

**样本不平衡导致的危害：**样本不均衡将导致样本量少的那一类样本所包含的特征信息太少，很难从中提取规律；即使得到分类模型，也容易产生过度依赖与有限的数据样本而导致过拟合问题。当模型应用到新的数据上时，模型的准确性会很差。

**面试回答：**可以从三个方面入手：

1. 从数据角度：主要方法是采样（随机欠采样大类别样本集和随机过采样小类别样本集）
2. 从算法角度：可以尝试不同的分类算法，如**决策树算法**往往在样本不均衡数据上表现不错。它是使用基于类变量的划分规则去创建分类树，因此可以强制的将不同类别分开。
3. 从评价指标角度：准确度这个评价指标在类别不均衡的分类任务中不能work，甚至进行误导。可以使用精确率、召回率、F1。



**扩展：解决方法：**

1. **扩大数据集：**

   增加数据（一定要有小类样本数据），更多的数据往往战胜更好的算法。 即使再增加小类样本数据时，又增加了大类样本数据，也可以使用放弃一部分大类数据（即对大类数据进行欠采样）来解决。 

2. **尝试其他评价指标：**

   准确度这个评价指标在类别不均衡的分类任务中不能work，甚至进行误导。可以使用精确率、召回率、F1

3. **对数据集进行重采样：**

   - 过抽样（over-sampling）小类样本（不足一万、甚至更少）：

     **扩充小类产生新数据。**SMOTE（Synthetic Minority Over-Sampling Technique）:其思想是合成新的少数类样本，合成的策略是对每个少数样本a，从他的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为合成的少数类样本。

   - 欠抽样（under-sampling）大类样本（超过1万、十万甚至更多）：

     **压缩大类，产生新数据。**设小类中有N个样本。将大类样本聚类成N个簇，然后使用每个簇的中心组成大类中的N个样本，加上小类中所有的样本进行训练。

4. **尝试产生人工数据样本：**

   一种简单的人工数据样本产生方法是：对该类下的所有样本每个属性特征的取值空间中随机取一个组成新的样本，即属性值随机采样。 你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。  

5. **尝试不同的分类算法：**

   决策树算法往往在样本不均衡的数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制的将不同的类别分开。

6. **尝试对模型进行惩罚：**

   可以使用相同的分类方法，使用不同的角度（如分类任务是识别小类），则可以对分类器的小类样本数据增加权重，降低大类样本的权重（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使分类器的重点集中在小类样本身上。具体做法就是在训练分类器时，如果分类器将小类样本分错时，额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加关心小类样本。如penalized-SVM何penalized-LDA算法。（即对分错的样本进行成惩罚。）

7. **尝试一个新的角度理解问题：**

   可以从不同于分类的角度解决数据不均衡性问题，我们可以将那些小类样本作为异常点（outliers），因此问题就可以转化为**异常点检测（anomaly detection）**与**变化趋势检测问题（change detection）**。

   （1）异常点检测：对那些罕见的事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。

   （2）变化趋势检测：类似于异常点检测，不同在于它是通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为中的不寻常改变。

8. **尝试创新：**

   仔细对问题进行分析与挖掘，是否可以将你的问题划分成多个更小的问题，而这些问题更容易解决。

   （1）将大类压缩成小类；

   （2）使用One Class分类器（将小类作为异常点）

   （3）使用集成方式，训练多个分类器，然后联合这些分类器进行分类。

**整体参考：**  https://blog.csdn.net/qq_33472765/article/details/86561557 

## 3. 机器学习中常用的分类器有哪些？

**先确定是回归还是分类。**两者的区别在于输出变量的类型。

- **回归**是定量输出：连续变量预测，根据训练集推断它所对应的输出值（实数）是多少。目的：是找到最优的拟合函数。评价方法是SSE（sum of square errors）.
- **分类**是定性输出：离散变量预测，根据训练集推断它所对应的类别。目的：是寻找决策边界。评价方法是精度、混淆矩阵。

李航老师《统计学习算法》中的定义：

输出变量为有限个离散变量的预测问题是分类问题；

输入变量和输出变量均为连续变量的预测问题是回归问题；

输入变量和输出变量均为变量序列的预测问题是标注问题。

**常用的分类器有：**

1. **线性回归**：根据训练数据集拟合一条直线或者曲线，反映数据的分布。

   评价准则或损失函数：所有预测值yi及对应值y之间的距离之和，使其最小化。

2. **逻辑回归**：是一种二分类方法，不是回归算法。

   它可以把输出的值映射到0-1之间表示概率问题，如果中间设定某一阈值（比如0.5），大于0.5的表示正类，小于0.5的表示负类，即二分类问题。

   **softmax回归**：跟逻辑回归一样，是分类算法而不是回归算法，只不过softmax针对的是多分类问题。

3. **SVM（支持向量机）**：定义在特征空间上的线性分类器，是一种二分类模型；

   超平面：分类的决策边界；

   支持向量：在SVM中，希望找到离分隔超平面最近的点，确保他们离分隔超平面的距离尽可能的远。

4. **贝叶斯分类器：**参考： https://www.cnblogs.com/NewBee-CHH/p/9770914.html 

5. **KNN（K近邻）**：是一种有监督的分类算法，前提是训练的数据集有类别标签；

   - （1）计算测试数据与各个训练数据之间的距离；
   - （2）按照距离的递增关系进行排序；
   - （3）选取距离最小的k个点；
   - （4）确定前k个点所在类别的出现频率；
   - （5）返回前k个点中出现频率最高的类别作为测试数据的预测分类。

6. **决策树：**

7. **集成分类器：**bagging和boosting两种集成方法。

   bagging代表是RF；

   boosting代表是GBDT；