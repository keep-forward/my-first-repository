# 高频问题

## 1. 非CS专业（Computer Science,计算机科学）为什么要做机器学习行业？

之前在一本书中有看到过一句话：机器学习产生于控制科学。而我的专业就是控制理论于控制工程，就是控制科学的专业。

## 2. 样本不平衡的解决方法 

**什么是样本不均衡：** 在处理文本分类问题经常会遇到文本不均衡的问题，即训练集中可能会存在某个或者类别下的样本数远大于另一些类别下的样本数目。比如正负样本1:10（类别不平衡比例超过4:1，就会造成偏移） 。

**样本不平衡导致的危害：**样本不均衡将导致样本量少的那一类样本所包含的特征信息太少，很难从中提取规律；即使得到分类模型，也容易产生过度依赖与有限的数据样本而导致过拟合问题。当模型应用到新的数据上时，模型的准确性会很差。

**面试回答：**可以从三个方面入手：

1. 从数据角度：主要方法是采样（随机欠采样大类别样本集和随机过采样小类别样本集）
2. 从算法角度：可以尝试不同的分类算法，如**决策树算法**往往在样本不均衡数据上表现不错。它是使用基于类变量的划分规则去创建分类树，因此可以强制的将不同类别分开。
3. 从评价指标角度：准确度这个评价指标在类别不均衡的分类任务中不能work，甚至进行误导。可以使用精确率、召回率、F1。



**扩展：解决方法：**

1. **扩大数据集：**

   增加数据（一定要有小类样本数据），更多的数据往往战胜更好的算法。 即使再增加小类样本数据时，又增加了大类样本数据，也可以使用放弃一部分大类数据（即对大类数据进行欠采样）来解决。 

2. **尝试其他评价指标：**

   准确度这个评价指标在类别不均衡的分类任务中不能work，甚至进行误导。可以使用精确率、召回率、F1

3. **对数据集进行重采样：**

   - 过抽样（over-sampling）小类样本（不足一万、甚至更少）：

     **扩充小类产生新数据。**SMOTE（Synthetic Minority Over-Sampling Technique）:其思想是合成新的少数类样本，合成的策略是对每个少数样本a，从他的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为合成的少数类样本。

   - 欠抽样（under-sampling）大类样本（超过1万、十万甚至更多）：

     **压缩大类，产生新数据。**设小类中有N个样本。将大类样本聚类成N个簇，然后使用每个簇的中心组成大类中的N个样本，加上小类中所有的样本进行训练。

4. **尝试产生人工数据样本：**

   一种简单的人工数据样本产生方法是：对该类下的所有样本每个属性特征的取值空间中随机取一个组成新的样本，即属性值随机采样。 你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。  

5. **尝试不同的分类算法：**

   决策树算法往往在样本不均衡的数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制的将不同的类别分开。

6. **尝试对模型进行惩罚：**

   可以使用相同的分类方法，使用不同的角度（如分类任务是识别小类），则可以对分类器的小类样本数据增加权重，降低大类样本的权重（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使分类器的重点集中在小类样本身上。具体做法就是在训练分类器时，如果分类器将小类样本分错时，额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加关心小类样本。如penalized-SVM何penalized-LDA算法。（即对分错的样本进行成惩罚。）

7. **尝试一个新的角度理解问题：**

   可以从不同于分类的角度解决数据不均衡性问题，我们可以将那些小类样本作为异常点（outliers），因此问题就可以转化为**异常点检测（anomaly detection）**与**变化趋势检测问题（change detection）**。

   （1）异常点检测：对那些罕见的事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。

   （2）变化趋势检测：类似于异常点检测，不同在于它是通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为中的不寻常改变。

8. **尝试创新：**

   仔细对问题进行分析与挖掘，是否可以将你的问题划分成多个更小的问题，而这些问题更容易解决。

   （1）将大类压缩成小类；

   （2）使用One Class分类器（将小类作为异常点）

   （3）使用集成方式，训练多个分类器，然后联合这些分类器进行分类。

**整体参考：**  https://blog.csdn.net/qq_33472765/article/details/86561557 

## 3. 机器学习中常用的分类器有哪些？

**先确定是回归还是分类。**两者的区别在于输出变量的类型。

- **回归**是定量输出：连续变量预测，根据训练集推断它所对应的输出值（实数）是多少。目的：是找到最优的拟合函数。评价方法是SSE（sum of square errors）.
- **分类**是定性输出：离散变量预测，根据训练集推断它所对应的类别。目的：是寻找决策边界。评价方法是精度、混淆矩阵。

李航老师《统计学习算法》中的定义：

输出变量为有限个离散变量的预测问题是分类问题；

输入变量和输出变量均为连续变量的预测问题是回归问题；

输入变量和输出变量均为变量序列的预测问题是标注问题。

**常用的分类器有：**

1. **线性回归**：根据训练数据集拟合一条直线或者曲线，反映数据的分布。

   评价准则或损失函数：所有预测值yi及对应值y之间的距离之和，使其最小化。

2. **逻辑回归**：是一种二分类方法，不是回归算法。

   它可以把输出的值映射到0-1之间表示概率问题，如果中间设定某一阈值（比如0.5），大于0.5的表示正类，小于0.5的表示负类，即二分类问题。

   **softmax回归**：跟逻辑回归一样，是分类算法而不是回归算法，只不过softmax针对的是多分类问题。

3. **SVM（支持向量机）**：定义在特征空间上的线性分类器，是一种二分类模型；

   超平面：分类的决策边界；

   支持向量：在SVM中，希望找到离分隔超平面最近的点，确保他们离分隔超平面的距离尽可能的远。

4. **贝叶斯分类器：**参考： https://www.cnblogs.com/NewBee-CHH/p/9770914.html 

5. **KNN（K近邻）**：是一种有监督的分类算法，前提是训练的数据集有类别标签；

   - （1）计算测试数据与各个训练数据之间的距离；
   - （2）按照距离的递增关系进行排序；
   - （3）选取距离最小的k个点；
   - （4）确定前k个点所在类别的出现频率；
   - （5）返回前k个点中出现频率最高的类别作为测试数据的预测分类。

6. **决策树：**

7. **集成分类器：**bagging和boosting两种集成方法。

   bagging代表是RF；

   boosting代表是GBDT；

## 4. 介绍树模型和熵



## 5. 为什么xgboost效果好？



## 6. 线性回归解析解的推导（三种方法），SVD与PCA的关系？



## 7. 深度学习与传统机器学习的区别？为什么深度学习效果这么好？

- **机器学习：**通常分为有监督学习、无监督学习和强化学习。解析数据，并根据从数据中学习到的知识做出决策。

- **深度学习：**深度学习可以看做是特殊的机器学习。利用各个层（输入层、隐藏层和输出层）组合创建人工“神经网络”，它能够智能的学习和做出决策。深度学习可以说是机器学习的子领域。

- **两者区别：**六大区别：数据依赖、硬件依赖、特征工程、解决问题方案、执行时间、可解释性。

  1. **数据依赖：**

     深度学习和传统机器学习的**主要区别在于性能。**随着数据量的增加，深度学习的性能也随之提高。当数据量很小的时候，深度学习的性能并不好，因为**深度学习算法需要大量的数据才能完全理解它。**

      ![深度学习和机器学习的六个本质区别你知道几个？](http://file.elecfans.com/web1/M00/AF/2B/o4YBAF3h3pmADVkjAADKFs3RsA0109.jpg) 

  2. **硬件依赖：**

     深度学习算法在很大深度上依赖高端机器。因为它对GPU有较高的要求，GPU是其工作的一个组成部分，因为它需要固定的执行大量的矩阵乘法运算。

     而机器学习可以在低端机器上工作，对硬件配置没有很高的要求。

  3. **特征工程：**

     特征工程是将领域知识应用到特征抽取的创建过程，以降低数据的复杂性为目的。但这一过程在训练时间和如何提取特征方面十分困难。

     **深度学习算法则试图从数据中学习更高级的特性。这是深度学习一个非常独特的部分，也是有别于传统机器学习的一部分。**而这种方式相较于机器学习，在训练时间和成本上有较高的提升。

  4. **解决问题方案：**

     传统机器学习算法在解决问题时，通常的做法是将问题分解成不同的部分，然后单独解决，最后结合起来得到结果。

     而深度学习则是更加提倡端到端地解决问题，直接给出问题的答案。

     **示例：**

     对图片中的多个目标进行探测，识别这些目标是什么，并确定他们分别在图片中的位置。

     机器学习算法的处理方法是：将问题分为两个部分：目标检测和目标识别。

     而深度学习算法的处理方法是：将图片传入YOLO网络（一种深度学习算法）直接给出对象的名称和具体位置。

  5. **执行时间：**

     深度学习算法需要很长的时间来训练，因为在深度学习算法中有太多的参数。通常需要好几天甚至好几周的时间。

     而机器学习算法所需要的训练时间就少得多，从几秒到几个小时不等。

  6. **可解释性：**

     深度学习算法效果是非常出色的，虽然在数学上可以发现神经网络中哪些结点被激活，但是我们不知道每个神经元、神经元层在做什么，无法对结果做出解释，相当于黑盒子一样在使用，这也是深度学习很难在工业中取得大规模应用的主要原因。

     相反，如决策树这样的机器学习算法为我们提供了清晰的规则，告诉了我们什么是它的选择以及为什么选择了它，很容易解释算法背后的推理。因此，决策树和线性/逻辑回归等机器学习算法主要用于工业中需要可解释性的场景。

- **为什么深度学习效果好？**

  1. 在模型训练的时候，对特征抽取的集成（之前特征抽取被视为是一个独立的问题）；
  2. 深度学习在过去未解决的问题上有成功的表现，源于它在开发过程中收集大数据集，以及对效能评估的系统性集成。
  3. 计算能力的飞速发展。

## 8. 深度学习的使用场景？

- 智能驾驶：识别路线、驾驶决策等
- 金融行业：智能核保OCR识别、车辆智能定损等
- 电商行业：货品分类、货品说明等
- 农业：作物识别、成熟度判别等
- 工业制造：残缺点检测、特征点标注等
- 人脸识别：人脸关键点定位、人脸身份认证、人脸属性、人脸聚类等。

## 9. 贪婪在机器学习中有哪些应用？（待完善）

1. 感知器

2. 决策树模型

   决策树在寻找分裂点的时候，用到力贪婪算法的思想。因为无法做到全局最优（这一层的最佳分裂，可能对下一层是负向的），所以会采用后剪枝来处理掉负向的分裂。

3. 一部分特征选择算法

## 10. 归一化的本质是什么？

**归一化的本质是由于loss函数不同造成的。**SVM用了欧氏距离，如果一个特征很大，就会把其他的维度dominated（控制，影响）。而LR可以通过权重调整使得损失函数不变。

之所以进行归一化是因为各维度的量纲不同。而且需要看情况归一化。

有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化；

有些模型伸缩后与原来等价，如LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果函数太扁迭代算法会发生不收敛的情况，所以最好归一化。

**扩展：**归一化和标准化的区别：

- 归一化（normalization）：

   ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BX_i-X_%7Bmin%7D%7D%7BX_%7Bmax%7D-X_%7Bmin%7D%7D) ,其中 ![[公式]](https://www.zhihu.com/equation?tex=X_%7Bmax%7D) 为最大值， ![[公式]](https://www.zhihu.com/equation?tex=X_%7Bmin%7D) 为最小值。

- 标准化（standardization）：

   ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BX_i-%5Cmu%7D%7B%5Csigma%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 代表样本的均值和标准差。

- 归一化和标准化的区别：

  - 归一化：缩放仅仅跟最大、最小值的差别有关。输出范围在0-1之间。
  - 标准化：缩放和每个点都有关系， 通过方差（variance）体现出来。与归一化对比，标准化中所有数据点都有贡献（通过均值和标准差造成影响）。 输出范围在负无穷到正无穷。

- 什么时候用归一化？什么时候用标准化？

  - 如果对输出结果范围有要求，用归一化；

  - 如果数据较为稳定，不存在极端的最大最小值，用归一化；

  - 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。

    

## 11. 数据结构与算法